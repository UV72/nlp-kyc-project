{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81a195dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Study\\Utility\\Anaconda\\conda\\envs\\idisc3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dependencies loaded\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "print(\"dependencies loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e31afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Study\\Utility\\Anaconda\\conda\\envs\\idisc3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processor and model loaded\n",
      "prompt added : <s_docvqa><s_question>What are the main numbers in this?</s_question><s_answer>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
    "\n",
    "# quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n",
    "processor = AutoProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "model = AutoModelForVision2Seq.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "print(\"processor and model loaded\")\n",
    "\n",
    "# task_prompt = \"<s_cord-v2>\"\n",
    "# task_prompt = \"<s_docvqa><s_question>Give 3 sets of 12 digit numbers</s_question><s_answer>\" # Gives 8800 and 0101 separately but not full number\n",
    "# task_prompt = \"<s_docvqa><s_question>Give a number in XXXX XXXX XXXX format</s_question><s_answer>\" # Gives full phone number\n",
    "# task_prompt = \"<s_docvqa><s_question>14 characters with 12 numbers and 2 blank space</s_question><s_answer>\"\n",
    "task_prompt = \"<s_docvqa><s_question>What are the main numbers in this?</s_question><s_answer>\"\n",
    "decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(device)\n",
    "print(f\"prompt added : {task_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e787db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r\"Custom-Generated Synthetic Aadhar Card Dataset for Robust Identity Authentication Research\\new_generated_aadharcard_images\"\n",
    "output_json = \"aadhar_parsed_results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189462e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Collect all image file paths\n",
    "valid_exts = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "image_paths = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.lower().endswith(valid_exts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34fbfec",
   "metadata": {},
   "source": [
    "# Full JSON extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f565a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "# Use DonutProcessor + VisionEncoderDecoderModel\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "MODEL_ID = \"sourinkarmakar/kyc_v1-donut-demo\"\n",
    "IMAGE_FOLDER = r\"Custom-Generated Synthetic Aadhar Card Dataset for Robust Identity Authentication Research\\new_generated_aadharcard_images\"\n",
    "NUM_TEST = 5   # process only first 5 images\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# --------------------------\n",
    "# Device\n",
    "# --------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# --------------------------\n",
    "# Load model + processor\n",
    "# --------------------------\n",
    "print(f\"Loading processor & model from: {MODEL_ID} ...\")\n",
    "processor = DonutProcessor.from_pretrained(MODEL_ID)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL_ID)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded.\\n\")\n",
    "\n",
    "# --------------------------\n",
    "# Prepare image list (first NUM_TEST)\n",
    "# --------------------------\n",
    "image_dir = Path(IMAGE_FOLDER)\n",
    "if not image_dir.exists():\n",
    "    raise FileNotFoundError(f\"Image folder not found: {image_dir}\")\n",
    "\n",
    "image_files = sorted([p for p in image_dir.iterdir() if p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"]])\n",
    "if len(image_files) == 0:\n",
    "    raise FileNotFoundError(\"No image files found in folder.\")\n",
    "\n",
    "image_files = image_files[:NUM_TEST]\n",
    "print(f\"Processing {len(image_files)} images (first {NUM_TEST}).\\n\")\n",
    "\n",
    "# --------------------------\n",
    "# Task prompt: try kyc-specific prompt, fallback to generic\n",
    "# --------------------------\n",
    "# This model was trained for KYC; its repo may use task-specific prompts.\n",
    "# Try a KYC-style prompt first; if output doesn't parse, we will show raw text.\n",
    "task_prompts_to_try = [ \"<s_cord-v2>\"]  # ordered guesses\n",
    "\n",
    "# Precompute decoder input ids for each prompt (put on device)\n",
    "decoder_input_map = {}\n",
    "for tp in task_prompts_to_try:\n",
    "    if tp == \"\":\n",
    "        # empty prompt\n",
    "        decoder_input_map[tp] = None\n",
    "    else:\n",
    "        decoder_input_map[tp] = processor.tokenizer(tp, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# --------------------------\n",
    "# Inference loop\n",
    "# --------------------------\n",
    "for img_path in tqdm(image_files, desc=\"KYC Donut inference\"):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Image:\", img_path.name)\n",
    "    print(\"=\" * 80)\n",
    "    try:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to open image {img_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Preprocess (pixel_values)\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    success = False\n",
    "    for task_prompt in task_prompts_to_try:\n",
    "        try:\n",
    "            decoder_input_ids = decoder_input_map[task_prompt]\n",
    "\n",
    "            # Generate\n",
    "            gen_kwargs = dict(\n",
    "                pixel_values=pixel_values,\n",
    "                max_length=MAX_LENGTH,\n",
    "                pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "            )\n",
    "            if decoder_input_ids is not None:\n",
    "                gen_kwargs[\"decoder_input_ids\"] = decoder_input_ids\n",
    "\n",
    "            outputs = model.generate(**gen_kwargs)\n",
    "\n",
    "            # Raw sequence (includes special tokens)\n",
    "            raw_seq = processor.batch_decode(outputs.sequences, skip_special_tokens=False)[0]\n",
    "            # Cleaned text (skip special tokens)\n",
    "            cleaned = processor.batch_decode(outputs.sequences, skip_special_tokens=True)[0]\n",
    "\n",
    "            print(f\"\\n--- Attempt with prompt: '{task_prompt or '<empty>'}' ---\")\n",
    "            print(\"\\nRaw sequence (with special tokens):\")\n",
    "            print(raw_seq)\n",
    "            print(\"\\nCleaned text (skip special tokens):\")\n",
    "            print(cleaned)\n",
    "\n",
    "            # Try the convenient token2json if available on the processor\n",
    "            parsed = None\n",
    "            try:\n",
    "                # Some processors expose token2json helper to decode Donut-style output to JSON\n",
    "                parsed = processor.token2json(cleaned)\n",
    "            except Exception:\n",
    "                # fallback: try to locate JSON substring and json.loads\n",
    "                try:\n",
    "                    # find first { and last }\n",
    "                    first = cleaned.find(\"{\")\n",
    "                    last = cleaned.rfind(\"}\")\n",
    "                    if first != -1 and last != -1 and last > first:\n",
    "                        json_str = cleaned[first:last + 1]\n",
    "                        parsed = json.loads(json_str)\n",
    "                except Exception:\n",
    "                    parsed = None\n",
    "\n",
    "            if parsed is not None:\n",
    "                print(\"\\n--- Parsed JSON ---\")\n",
    "                print(json.dumps(parsed, indent=4, ensure_ascii=False))\n",
    "                success = True\n",
    "                break\n",
    "            else:\n",
    "                print(\"\\n Could not parse a JSON object from this prompt's output. Showing raw/cleaned output above.\")\n",
    "                # continue trying other prompts\n",
    "\n",
    "        except Exception as gen_e:\n",
    "            print(f\"\\nGeneration failed for prompt '{task_prompt}': {gen_e}\")\n",
    "            # try next prompt\n",
    "\n",
    "    if not success:\n",
    "        print(\"\\n No JSON parsed from any prompt for this image. Save raw cleaned text for manual inspection.\")\n",
    "        print(\"Final cleaned text (last attempt):\")\n",
    "        try:\n",
    "            print(cleaned)\n",
    "        except NameError:\n",
    "            print(\"<no cleaned text available>\")\n",
    "\n",
    "    # Optional: token-level scores / logits (if outputs.scores exists)\n",
    "    try:\n",
    "        if hasattr(outputs, \"scores\") and outputs.scores is not None:\n",
    "            print(\"\\n--- Token-level top token & confidence per generation step ---\")\n",
    "            for i, step_logits in enumerate(outputs.scores):\n",
    "                # compute softmax to get probabilities\n",
    "                probs = torch.nn.functional.softmax(step_logits, dim=-1)\n",
    "                top_idx = torch.argmax(probs, dim=-1).item()\n",
    "                top_token = processor.tokenizer.decode([top_idx])\n",
    "                top_prob = probs[0, top_idx].item() if probs.dim() == 2 else probs[top_idx].item()\n",
    "                print(f\"Step {i:03d}: token='{top_token}'  prob={top_prob:.4f}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7873142",
   "metadata": {},
   "source": [
    "# QA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40664bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images. Running test inference on first 10 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Aadhaar test images:  10%|█         | 1/10 [00:06<00:55,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 100backside_blurred.jpg:\n",
      "{\n",
      "    \"raw_text\": \"<s_docvqa><s_question> What are the main numbers in this?</s_question><s_answer> 8800</s_answer>\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Aadhaar test images:  20%|██        | 2/10 [00:09<00:34,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 100backside_contrast_adjusted.jpg:\n",
      "{\n",
      "    \"raw_text\": \"<s_docvqa><s_question> What are the main numbers in this?</s_question><s_answer> 8800 6030 0101</s_answer>\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Aadhaar test images:  30%|███       | 3/10 [00:12<00:27,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 100backside_hue_sat_adjusted.jpg:\n",
      "{\n",
      "    \"raw_text\": \"<s_docvqa><s_question> What are the main numbers in this?</s_question><s_answer> 8800 6030 0101</s_answer>\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Aadhaar test images:  40%|████      | 4/10 [00:15<00:22,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 100backside_scaled_down.jpg:\n",
      "{\n",
      "    \"raw_text\": \"<s_docvqa><s_question> What are the main numbers in this?</s_question><s_answer> 8800 6030 0101</s_answer>\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Aadhaar test images:  50%|█████     | 5/10 [00:19<00:17,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 100backside_scaled_up.jpg:\n",
      "{\n",
      "    \"raw_text\": \"<s_docvqa><s_question> What are the main numbers in this?</s_question><s_answer> 8800 6030 0101</s_answer>\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Aadhaar test images:  60%|██████    | 6/10 [00:22<00:14,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 100front_blurred.jpg:\n",
      "{\n",
      "    \"raw_text\": \"<s_docvqa><s_question> What are the main numbers in this?</s_question><s_answer> 8800</s_answer>\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Aadhaar test images:  70%|███████   | 7/10 [00:26<00:10,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 100front_contrast_adjusted.jpg:\n",
      "{\n",
      "    \"raw_text\": \"<s_docvqa><s_question> What are the main numbers in this?</s_question><s_answer> 3Hetr</s_answer>\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Aadhaar test images:  80%|████████  | 8/10 [00:29<00:07,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 100front_hue_sat_adjusted.jpg:\n",
      "{\n",
      "    \"raw_text\": \"<s_docvqa><s_question> What are the main numbers in this?</s_question><s_answer> 3Hetr</s_answer>\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Aadhaar test images:  90%|█████████ | 9/10 [00:33<00:03,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 100front_scaled_down.jpg:\n",
      "{\n",
      "    \"raw_text\": \"<s_docvqa><s_question> What are the main numbers in this?</s_question><s_answer> 3Hetr</s_answer>\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Aadhaar test images: 100%|██████████| 10/10 [00:36<00:00,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 100front_scaled_up.jpg:\n",
      "{\n",
      "    \"raw_text\": \"<s_docvqa><s_question> What are the main numbers in this?</s_question><s_answer> 3Hetr</s_answer>\"\n",
      "}\n",
      "\n",
      "✅ Test inference completed for first 10 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "print(f\"Found {len(image_paths)} images. Running test inference on first 10 images...\")\n",
    "\n",
    "# Limit to first 5 images\n",
    "test_images = image_paths[:10]\n",
    "\n",
    "for img_path in tqdm(test_images, desc=\"Processing Aadhaar test images\"):\n",
    "    try:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Preprocess image\n",
    "        pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "        # Generate output\n",
    "        outputs = model.generate(\n",
    "            pixel_values,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            max_length=model.decoder.config.max_position_embeddings,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "\n",
    "        # Decode text sequence\n",
    "        sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "        sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "        sequence = sequence.replace(task_prompt, \"\").strip()\n",
    "\n",
    "        # Try to parse JSON output (Donut outputs structured text)\n",
    "        try:\n",
    "            parsed = json.loads(sequence)\n",
    "        except json.JSONDecodeError:\n",
    "            parsed = {\"raw_text\": sequence}\n",
    "\n",
    "        print(f\"\\n {os.path.basename(img_path)}:\")\n",
    "        print(json.dumps(parsed, indent=4, ensure_ascii=False))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "print(\"\\nTest inference completed for first 10 images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd5f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.autonotebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(f\"Found {len(image_paths)} images. Starting inference...\")\n",
    "\n",
    "for img_path in tqdm(image_paths, desc=\"Processing Aadhaar images\"):\n",
    "    try:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Preprocess image\n",
    "        pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "        # Generate output\n",
    "        outputs = model.generate(\n",
    "            pixel_values,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            max_length=model.decoder.config.max_position_embeddings,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "\n",
    "        # Decode text sequence\n",
    "        sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "        sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "        sequence = sequence.replace(task_prompt, \"\").strip()\n",
    "\n",
    "        # Try to parse JSON output (Donut outputs structured text)\n",
    "        try:\n",
    "            parsed = json.loads(sequence)\n",
    "        except json.JSONDecodeError:\n",
    "            parsed = {\"raw_text\": sequence}\n",
    "\n",
    "        results[os.path.basename(img_path)] = parsed\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "#  Save results\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n Inference completed. Results saved to: {output_json}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idisc3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
